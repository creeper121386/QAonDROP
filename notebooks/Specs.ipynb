{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from allennlp.data.dataset_readers.reading_comprehension.util import split_tokens_by_hyphen\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_answer_type(answers):\n",
    "    if answers['number']:\n",
    "        return 'number'\n",
    "    elif answers['spans']:\n",
    "        if len(answers['spans']) == 1:\n",
    "            return 'single_span'\n",
    "        return 'multiple_span'\n",
    "    elif any(answers['date'].values()):\n",
    "        return 'date'\n",
    "\n",
    "def get_specs(file_path, tokenizer, wordpiece_tokenizer):\n",
    "    with open(file_path) as dataset_file:\n",
    "        dataset = json.load(dataset_file)\n",
    "        \n",
    "    num_passage_words = 0\n",
    "    num_passage_tokens = 0\n",
    "    passage_count = 0\n",
    "    num_question_words = 0\n",
    "    num_question_tokens = 0\n",
    "    question_count = 0\n",
    "    num_answers = 0\n",
    "    passage_vocab = set()\n",
    "    passage_vocab_token = set()\n",
    "    question_vocab = set()\n",
    "    question_vocab_token = set()\n",
    "    answer_counts = defaultdict(int)\n",
    "    \n",
    "    \n",
    "    for passage_id, passage_info in tqdm(dataset.items()):\n",
    "        passage_text = passage_info[\"passage\"].strip()\n",
    "\n",
    "                \n",
    "        passage_words = [x.text for x in split_tokens_by_hyphen(tokenizer.tokenize(passage_text))]\n",
    "        passage_tokens = wordpiece_tokenizer.tokenize(passage_text)\n",
    "        \n",
    "        num_passage_words += len(passage_words)\n",
    "        num_passage_tokens += len(passage_tokens)\n",
    "        passage_vocab.update(passage_words)\n",
    "        passage_vocab_token.update(passage_tokens)\n",
    "        passage_count += 1\n",
    "        \n",
    "        # Process questions from this passage\n",
    "        for question_answer in passage_info[\"qa_pairs\"]:\n",
    "            question_id = question_answer[\"query_id\"]\n",
    "            question_text = question_answer[\"question\"].strip()\n",
    "            \n",
    "            question_words = [x.text for x in split_tokens_by_hyphen(tokenizer.tokenize(question_text))]\n",
    "            question_tokens = wordpiece_tokenizer.tokenize(question_text)\n",
    "        \n",
    "            num_question_words += len(question_words)\n",
    "            num_question_tokens += len(question_tokens)\n",
    "            question_vocab.update(question_words)\n",
    "            question_vocab_token.update(question_tokens)\n",
    "            question_count += 1\n",
    "            \n",
    "            answer_annotations = []\n",
    "            if \"answer\" in question_answer:\n",
    "                answer_annotations.append(question_answer[\"answer\"])\n",
    "                answer_type = get_answer_type(question_answer[\"answer\"])\n",
    "                answer_counts[get_answer_type(question_answer[\"answer\"])] += 1\n",
    "            if \"validated_answers\" in question_answer:\n",
    "                answer_annotations += question_answer[\"validated_answers\"]\n",
    "            \n",
    "            num_answers += len(answer_annotations)\n",
    "            \n",
    "    print(float(num_passage_words) / passage_count) \n",
    "    print(float(num_passage_tokens) / passage_count)\n",
    "    print(float(num_question_words) / question_count)\n",
    "    print(float(num_question_tokens) / question_count)\n",
    "    print(float(num_answers) / question_count)\n",
    "    print(float(question_count) / passage_count)\n",
    "    print(passage_count)\n",
    "    print(question_count)\n",
    "    print(len(passage_vocab))\n",
    "    print(len(passage_vocab_token))\n",
    "    print(len(question_vocab))\n",
    "    print(len(question_vocab_token))\n",
    "    print([(key, float(answer_counts[key]) / question_count) for key in answer_counts])\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_specs('data/drop_dataset_dev.json', WordTokenizer(), BertTokenizer.from_pretrained('bert-base-uncased'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_specs('data/drop_dataset_train.json', WordTokenizer(), BertTokenizer.from_pretrained('bert-base-uncased'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
